# -*- coding: utf-8 -*-
"""Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_xf6aRBeldzuDKMLMFgS4irm5tX67YJ

**Data PrepProcess of SALICON**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cd /content
# if [ ! -d Improving-the-Accuracy-of-Saliency-Prediction-models-using-Transfer-Learning ]; then
#   git clone https://github.com/Pouya-Ta/Improving-the-Accuracy-of-Saliency-Prediction-models-using-Transfer-Learning.git
# fi
#

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!ls -l ~/.kaggle

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p data/raw
!unzip -q "/content/drive/My Drive/salicon_subset.zip" -d data/raw/

!find data/raw -maxdepth 3 -print | sed 's/^/→ /'

!find data/raw -maxdepth 4 -print | sed 's/^/→ /'

!pip install pillow numpy scipy tqdm

import json, random, os
import numpy as np
from pathlib import Path
from PIL import Image, ImageOps
from scipy.ndimage import gaussian_filter
from tqdm import tqdm

# ─── PATHS ─────────────────────────────────────────────────────────
BASE       = Path("data/raw/salicon_subset")
IMG_DIR    = BASE/"images"/"train"
ANN_FILE   = BASE/"annotations"/"train_subset.json"
OUT_IMG    = Path("data/processed/images")
OUT_MAP    = Path("data/processed/maps")

# ─── MODEL & SPLIT SETTINGS ────────────────────────────────────────
TARGET_SZ  = (224, 224)                # VGG16 input size
MEAN       = np.array([0.485, 0.456, 0.406], np.float32)
STD        = np.array([0.229, 0.224, 0.225], np.float32)
SPLITS     = {"train":0.7, "val":0.15, "test":0.15}
SEED       = 42
# ───────────────────────────────────────────────────────────────────

# Create output directories
for split in SPLITS:
    (OUT_IMG/ split).mkdir(parents=True, exist_ok=True)
    (OUT_MAP/ split).mkdir(parents=True, exist_ok=True)

with open(ANN_FILE) as f:
    data = json.load(f)
images      = data["images"]
annotations = data["annotations"]
categories  = data.get("categories", [])

# Map image_id → list of its annotations
ann_map = {}
for ann in annotations:
    ann_map.setdefault(ann["image_id"], []).append(ann)

# Shuffle & split
random.seed(SEED)
random.shuffle(images)
N       = len(images)
n_train = int(N * SPLITS["train"])
n_val   = int(N * SPLITS["val"])

splits = {
    "train": images[:n_train],
    "val":   images[n_train:n_train + n_val],
    "test":  images[n_train + n_val:]
}

for split, imgs in splits.items():
    split_anns = [ann for img in imgs for ann in ann_map[img["id"]]]
    manifest = {"images": imgs, "annotations": split_anns, "categories": categories}
    out_path = Path("data/processed")/f"{split}_manifest.json"
    out_path.parent.mkdir(exist_ok=True)
    with open(out_path, "w") as wf:
        json.dump(manifest, wf, indent=2)
    print(f"{split.upper()}: {len(imgs)} images, {len(split_anns)} annotations")

"""**PreProcessing:**"""

def resize_and_pad(img, size=TARGET_SZ, fill=0):
    img.thumbnail(size, Image.BILINEAR)
    delta_w = size[0] - img.width
    delta_h = size[1] - img.height
    padding = (delta_w//2, delta_h//2, delta_w - delta_w//2, delta_h - delta_h//2)
    return ImageOps.expand(img, padding, fill=fill)

# Build fixation map dict once
fmap = {ann["image_id"]: np.array(ann["fixations"], np.float32) - 1
        for ann in data["annotations"]}

for split, imgs in splits.items():
    print(f"Preprocessing {split} ({len(imgs)} images)…")
    for img_meta in tqdm(imgs):
        fname = img_meta["file_name"]
        img_id= img_meta["id"]

        # — Image —
        orig = Image.open(IMG_DIR/fname).convert("RGB")
        padded_img = resize_and_pad(orig, TARGET_SZ, fill=0)
        arr = np.asarray(padded_img, np.float32) / 255.0
        arr = (arr - MEAN) / STD

        # — Saliency map —
        # 1) rasterize at original size
        H, W = img_meta["height"], img_meta["width"]
        heat = np.zeros((H, W), np.float32)
        pts  = fmap[img_id]
        ys   = pts[:,0].astype(int)
        xs   = pts[:,1].astype(int)
        for y, x in zip(ys, xs):
            if 0 <= y < H and 0 <= x < W:
                heat[y, x] += 1
        # 2) smooth & normalize
        heat = gaussian_filter(heat, sigma=5)
        if heat.max()>0: heat /= heat.max()
        # 3) pad to TARGET_SZ
        heat_img    = Image.fromarray((heat*255).astype(np.uint8))
        padded_heat = resize_and_pad(heat_img, TARGET_SZ, fill=0)
        heat_final  = np.asarray(padded_heat, np.float32) / 255.0

        # save
        base = fname.replace(".jpg", ".npy")
        np.save(OUT_IMG/ split/ base,      arr)
        np.save(OUT_MAP/ split/ base, heat_final)

from pathlib import Path

for split in ("train","val","test"):
    imgs = list((OUT_IMG/split).glob("*.npy"))
    maps = list((OUT_MAP/split).glob("*.npy"))
    print(f"{split:5s}: images → {len(imgs)}, maps → {len(maps)}")
    print(" Example image file:", imgs[:1])
    print(" Example map   file:", maps[:1])

import json, shutil
from pathlib import Path

RAW_IMG_DIR    = Path("data/raw/salicon_subset/images/train")
MANIFEST_DIR   = Path("data/processed")
OUT_SPLITS_DIR = Path("data/splits")

for split in ("train","val","test"):
    print(f"Exporting {split}…")
    manifest_path = MANIFEST_DIR / f"{split}_manifest.json"
    with open(manifest_path) as f:
        manifest = json.load(f)
    imgs = manifest["images"]

    out_img_dir = OUT_SPLITS_DIR / split / "images"
    out_img_dir.mkdir(parents=True, exist_ok=True)

    for img_meta in imgs:
        fname = img_meta["file_name"]
        src  = RAW_IMG_DIR / fname
        dst  = out_img_dir / fname
        if src.exists():
            shutil.copy(src, dst)
        else:
            print("⚠️ Missing:", src)

    out_json_path = OUT_SPLITS_DIR / split / "annotations.json"
    with open(out_json_path, "w") as wf:
        json.dump(manifest, wf, indent=2)

    print(f"  → {len(list(out_img_dir.iterdir()))} images, JSON → {out_json_path}")

!find data/splits -maxdepth 3 -print | sed 's/^/→ /'

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cd /content/Improving-the-Accuracy-of-Saliency-Prediction-models-using-Transfer-Learning
# 
# zip -r train_npys.zip data/processed/images/train data/processed/maps/train
# 
# zip -r val_npys.zip   data/processed/images/val   data/processed/maps/val
# 
# zip -r test_npys.zip  data/processed/images/test  data/processed/maps/test
#

!cp train_npys.zip "/content/drive/My Drive/"
!cp val_npys.zip   "/content/drive/My Drive/"
!cp test_npys.zip  "/content/drive/My Drive/"

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cd /content/Improving-the-Accuracy-of-Saliency-Prediction-models-using-Transfer-Learning
# 
# zip -r raw_salicon_subset.zip data/raw/salicon_subset
# 
# cp raw_salicon_subset.zip "/content/drive/My Drive/"
# 
# echo "✅ raw_salicon_subset.zip is now in your Drive root."
#